\chapter{Introduction}
blah blah blah
\section{Sequential versus parallel computing}
Developers worldwide are, and have always been, on the lookout for increased
computing performance.

Until recently, the increased performance could easily be
achieved through advances within raw computing power, as CPU's had steadily been
doubling their number of on-chip transistors, in rough accordance to Moore's Law (cit√©r
her).
However, the performance increases in CPU design has now slowed down
significantly, due to physical limitations to CPU design.

Instead of adding more transistors or increasing the clock frequency of newer
CPUs, the CPU manufacturers have instead opted to split their single-core CPUs
up into multicore CPUs, which means that any program can now run several threads
on the CPU's cores, simultaneously.
The CPU's cores are specialized in advanced computations 
's cores are 


%% udfyld om hvordan GPU'en har mange flere kerner, og kan bruges til relativt
%% simple operationer 

%% WHY ARE BIG DATA INTERESTING
In sectors like the financial sector and within the natural sciences, there is a
need for handling large amounts of data in an effective manner. With algorithmic
trading gaining ground within the trading sector, the trader who can analyze
incoming buy- and sell-offers the fastest, usually has the advantage at the
exchanges.
Likewise, faster computing can increase productivity for chemists and
biologists who are analysing large datasets, physicists can run faster
simulations, and so on.
All of these activities are based on executing relatively simple computations on enormous datasets.
The hundreds of simultaneous threads on the GPUs are, compared to the CPU,
optimal for performing these calculations as fast as possible,
which is why GPUs are increasingly being used for \textit{General Purpose
  Computing on Graphics Processing Units}.

\section{Parallel programming in Futhark}
GPU programming is in principle easily available for everyone. As long as the user has access
to a GPU and a reasonable PC for developing software, it just takes a bit of
effort and reading to get started with CUDA, OpenCL or similar programming.
Realistically however, it takes much more than just a little effort to start writing
one's own GPU programs.

Take the function $f(x) = ax+y$. In figure \ref{fig:cudasaxpy} we see the
function implemented as a CUDA program. In this program, we are defining the
kernel \texttt{saxpy} itself, and also manually copying data back and forth
between the GPU.
Now take the same program, written in Futhark as in figure \ref{fig:futsaxpy}.

Whereas the CUDA kernel needs to check whether the current thread is outside of
the bounds of the input data, the equivalent kernel in Futhark is simply a
declaration of it's function. Also, the Futhark version does not bother with
getting array elements by the current thread ID.

In the main function itself, the initial lists are generated by functions, and
the user doesn't have to allocate space on neither the computer \textit{host},
or the GPU \textit{device}.

Of course, Futhark is compiled into either C- or Python code that does indeed
contain memalloc calls, GPU kernels with bounds checking, and so on, but that
part is very well hidden from the Futhark programmers themselves.
All in all, writing effective GPU programs becomes much more accessible when it's
possible to do in a declarative manner, like Futhark, without also having to
the minute details that comes with GPU computations.
\begin{figure}
  \centering
\begin{minted}{cpp}
  #include <stdio.h>

__global__
void saxpy(int n, float a, float *x, float *y)
{
  int i = blockIdx.x*blockDim.x + threadIdx.x;
  if (i < n) y[i] = a*x[i] + y[i];
}

int main(void)
{
  int N = 1<<20;
  float *x, *y, *d_x, *d_y;
  x = (float*)malloc(N*sizeof(float));
  y = (float*)malloc(N*sizeof(float));

  cudaMalloc(&d_x, N*sizeof(float)); 
  cudaMalloc(&d_y, N*sizeof(float));

  for (int i = 0; i < N; i++) {
    x[i] = 1.0f;
    y[i] = 2.0f;
  }

  cudaMemcpy(d_x, x, N*sizeof(float), cudaMemcpyHostToDevice);
  cudaMemcpy(d_y, y, N*sizeof(float), cudaMemcpyHostToDevice);

  // Perform SAXPY on 1M elements
  saxpy<<<(N+255)/256, 256>>>(N, 2.0f, d_x, d_y);

  cudaMemcpy(y, d_y, N*sizeof(float), cudaMemcpyDeviceToHost);

  cudaFree(d_x);
  cudaFree(d_y);
  free(x);
  free(y);
}
\end{minted}
  \caption{$ax + y$ in CUDA}
  \label{fig:cudasaxpy}
\end{figure}

\begin{figure}
  \centering
  \begin[language=Futhark]{lstlisting}
    let saxpy (a : f32) (x : f32) (y : f32) : f32 = a*x+y
    let main =
      let N = 1<<20
      let a = 2f32
      let xs = replicate N 1f32
      let ys = replicate N 2f32
      let ys = map2 (saxpy a) xs ys
      in ys
  \end{lstlisting}
  \caption{$ax+y$ in Futhark}
  \label{fig:futsaxpy}
\end{figure}

\section{Motivation}

\section{The contributions of this thesis}

\section{Roadmap}








%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../thesis"
%%% End: