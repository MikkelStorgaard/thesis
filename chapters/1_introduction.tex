\chapter{Introduction}
blah blah blah
\section{Sequential versus parallel computing}
Developers worldwide are, and have always been, on the lookout for increased
computing performance.

Until recently, the increased performance could easily be
achieved through advances within raw computing power, as CPU's had steadily been
doubling their number of on-chip transistors, in rough accordance to Moore's Law (cit√©r
her).
However, the performance increases in CPU design has now slowed down
significantly, due to physical limitations to CPU design.

Instead of adding more transistors or increasing the clock frequency of newer
CPUs, the CPU manufacturers have instead opted to split their single-core CPUs
up into multicore CPUs, which means that any program can now run several threads
on the CPU's cores, simultaneously.
The CPU's cores are specialized in advanced computations 
's cores are 


%% udfyld om hvordan GPU'en har mange flere kerner, og kan bruges til relativt
%% simple operationer 

%% WHY ARE BIG DATA INTERESTING
In sectors like the financial sector and within the natural sciences, there is a
need for handling large amounts of data in an effective manner. With algorithmic
trading gaining ground within the trading sector, the trader who can analyze
incoming buy- and sell-offers the fastest, usually has the advantage at the
exchanges.
Likewise, faster computing can increase productivity for chemists and
biologists who are analysing large datasets, physicists can run faster
simulations, and so on.
All of these activities are based on executing relatively simple computations on enormous datasets.
The hundreds of simultaneous threads on the GPUs are, compared to the CPU,
optimal for performing these calculations as fast as possible,
which is why GPUs are increasingly being used for \textit{General Purpose
  Computing on Graphics Processing Units}.

\section{Parallel programming in Futhark}
GPU programming is in principle easily available for everyone. As long as the user has access
to a GPU and a reasonable PC for developing software, it just takes a bit of
effort and reading to get started with CUDA, OpenCL or similar programming.
Realistically however, it takes much more than just a little effort to start writing
one's own GPU programs.

Take the function $f(x) = ax+y$. In figure \ref{fig:cudasaxpy} we see the
function implemented as a CUDA program. In this program, we are defining the
kernel \texttt{saxpy} itself, and also manually copying data back and forth
between the GPU.
Now take the same program, written in Futhark as in figure \ref{fig:futsaxpy}.

Whereas the CUDA kernel needs to check whether the current thread is outside of
the bounds of the input data, the equivalent kernel in Futhark is simply a
declaration of it's function. Also, the Futhark version does not bother with
getting array elements by the current thread ID.

In the main function itself, the initial lists are generated by functions, and
the user doesn't have to allocate space on neither the computer \textit{host},
or the GPU \textit{device}.
The hard work is done by a Futhark SOAC, which is eventually compiled into a
kernel  %% MERE MERE MERE

Of course, Futhark is compiled into either C- or Python code that does indeed
contain memalloc calls, GPU kernels with bounds checking, and so on, but that
part is very well hidden from the Futhark programmers themselves.
All in all, writing effective GPU programs becomes much more accessible when it's
possible to do in a declarative manner, like Futhark, without also having to
the minute details that comes with GPU computations.
\begin{figure}
  \centering
\begin{minted}{cpp}
  #include <stdio.h>

__global__
void saxpy(int n, float a, float *x, float *y)
{
  int i = blockIdx.x*blockDim.x + threadIdx.x;
  if (i < n) y[i] = a*x[i] + y[i];
}

int main(void)
{
  int N = 1<<20;
  float *x, *y, *d_x, *d_y;
  x = (float*)malloc(N*sizeof(float));
  y = (float*)malloc(N*sizeof(float));

  cudaMalloc(&d_x, N*sizeof(float)); 
  cudaMalloc(&d_y, N*sizeof(float));

  for (int i = 0; i < N; i++) {
    x[i] = 1.0f;
    y[i] = 2.0f;
  }

  cudaMemcpy(d_x, x, N*sizeof(float), cudaMemcpyHostToDevice);
  cudaMemcpy(d_y, y, N*sizeof(float), cudaMemcpyHostToDevice);

  // Perform SAXPY on 1M elements
  saxpy<<<(N+255)/256, 256>>>(N, 2.0f, d_x, d_y);

  cudaMemcpy(y, d_y, N*sizeof(float), cudaMemcpyDeviceToHost);

  cudaFree(d_x);
  cudaFree(d_y);
  free(x);
  free(y);
}
\end{minted}
  \caption{$ax + y$ in CUDA}
  \label{fig:cudasaxpy}
\end{figure}

\begin{figure}
  \centering
  \begin{lstlisting}[language=Futhark]
    let saxpy (a : f32) (x : f32) (y : f32) : f32 =
      a*x+y

    let main =
      let N = 1<<20
      let a = 2f32
      let xs = replicate N 1f32
      let ys = replicate N 2f32
      let ys = map2 (saxpy a) xs ys
      in ys
  \end{lstlisting}
  \caption{$ax+y$ in Futhark}
  \label{fig:futsaxpy}
\end{figure}

\section{Motivation}
\fshark{} is intended to be a way of writing and utilizing Futhark, without
actually having to write or interact with the Futhark language and compiler
itself. Besides some tooling and an \fsharp{} SOAC library, it primarily consists of the \fshark{} compiler that compiles from
\fsharp{} source code to Futhark source code, and the Futhark \csharp{}
generator, which compiles Futhark programs as either standalone \csharp{}
programs or -libraries.

As much as most developers are happy to increase performance on big
computations, it is not always an option to incorporate an extra langauge into
an already existing programming language. At this moment, using Futhark in
either a \csharp{}- or \fsharp{} project is a contrived process that usually
requires spawning a subprocess with a \texttt{futhark-opencl} C program from inside one of the .NET
projects.

In order to use Futhark natively in .NET languages, it is therefore
necessary to write a backend for Futhark in a .NET language.
For \fshark{}, I have chosen to implement this backend in \csharp{}, as the Futhark intermediate
code \texttt{ImpCode}\footnote{which stands for Imperative Code} is trivial to
translate into imperative \csharp{} statements and expressions.
Also, there are \csharp{} libraries available which supply OpenCL bindings, which are
needed to implement the necessary OpenCL constructs from \texttt{ImpCode}.

It is my belief that exporting Futhark programs as .NET executables and
-libraries will lower the barrier to Futhark usage in .NET projects
significantly, hopefully increasing the all-round number of Futhark users, and
in the long term, increasing utilization of GPU programming and making it more
widely available.

However, one could do even more than just exporting Futhark to .NET, to increase
accessibility:

As tens of thousands of programmers worldwide (CHECK NUMBER JEEEEZ) are already
writing \fsharp{} programs, and that most of \fsharp{}s functional language features can be
directly translated into equivalent Futhark features, it became worthwhile to
investigate whether it was possible to design a way for users to both write and
utilize Futhark in \fsharp{} projects, without ever actually touching the
Futhark language or compiler themselves.
Instead, users can write their data parallel \fsharp{} modules in \fshark{}, and compile these
modules into Futhark libraries automatically.

In this case, it would be possible to get Futhark speeds in \fsharp{} programs,
without doing much more than installing the Futhark compiler locally, and adding
the required \fshark{} libraries to the \fsharp{} project.

It is my belief that being able to achieve Futhark performance in regular \fsharp{}
programs almost automatically, will make it significantly easier for people to
adapt to Futhark programming.

(SOME MORE MORE SOME MORE)

\section{The contributions of this thesis}
The contributions of this thesis are as follows:
\begin{enumerate}
\item The \texttt{FSharkPrelude}:\\
  The \texttt{FSharkPrelude} is a subset of the Futhark SOACs, ported to
  \fsharp{}. To write an \fshark{} program, the user is directed to limit
  himself to the SOACs in the \texttt{FSharkPrelude}. This means exchanging
  \texttt{Array.map} for \texttt{FSharkPrelude.Map}, \texttt{Array.foldBack} for
  \texttt{FSharkPrelude.foldr}, and so on.
  However, the \texttt{FSharkPrelude} carries the guarantee, that all
  \texttt{FSharkPrelude} functions works equivalently to their Futhark SOAC namesakes. 
  This prelude, together with the \fsharp{} subset chosen for \fshark{}, makes
  it possible to write \fsharp{} programs which, when translated to Futhark
  code, are equivalent to their Futhark counterparts.

\item An \fsharp{} subset translatable with \fshark{}:\\
  As \fsharp{} is not only a multi-paradigm language, but also has access to the
  entire standard .NET library, it was required to make \fshark{} support only a
  subset of \fsharp{}. This has been implemented by whitelisting only the
  \fsharp{}-to-Futhark translatable types, constructs and expressions in the \fshark{}
  compiler. Furthermore, no other module imports than \texttt{FSharkPrelude} are allowed. 
  This subset is of course documented for users.

\item The \fshark{} Compiler and Wrapper:\\
  The \fshark{} Compiler and Wrapper takes a module written in \fshark{} as
  input, converts the \fshark{} module into a compiled Futhark \csharp{} module,
  and makes it available to the \fsharp{} program, all at runtime.
  The pipeline is described in sec \ref{chapter:fsharkcompiler}

\item A \csharp{} backend for Futhark:\\
  To actually use Futhark in \csharp{} projects (and transitively \fsharp{}
  projects), it was necessary to develop and add a \csharp{} backend to
  the Futhark compiler. This backend is equivalent in functionality to
  the \texttt{C}- and the \texttt{Python} backends that are already available.

\end{enumerate}

\section{Roadmap}
The main part of this thesis is split in four parts.
blaaaah


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../thesis"
%%% End: