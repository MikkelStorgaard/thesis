\chapter{Background}
In this chapter we will first show two languages for GPU programming, namely
CUDA and Futhark. Then we will show \csharp{} and it's interoperability with
Futhark.
Finally, we will take a look at \fsharp{}, and how we can expect
Futhark/\fsharp{} interoperability.

\section{CUDA}
GPU programming is in principle easily available for everyone. As long as the user has access
to a GPU and a reasonable PC for developing software, it just takes a bit of effort and reading to get started with CUDA, OpenCL or similar programming.
Realistically however, it takes much more than just a little effort to start
writing one's own GPU programs. 

\subsection{A simple CUDA program}
Take for instance the function $f(x,y) = ax+y$. In figure \ref{fig:cudasaxpy} we see the
function implemented as a CUDA program. In this program, we are defining the
kernel \texttt{saxpy} itself, and also manually copying data back and forth
between the GPU.

\subsubsection{The CUDA kernel}
Line 3's \texttt{\_\_global\_\_} signifies that the following function is a CUDA kernel.
Line 4 to 10 contains the computational kernel for the GPU.
Line 4 contains the kernels name and arguments. The kernel takes as arguments
\textit{a} is a scalar constant, the pointers \textit{x} and \textit{y} are
references to floating point arrays in the GPU device memory, and \textit{n}
denotes the length of the arrays \textit{x} and \textit{y}.\\
Line 6 computes the current computational thread's global id $i$. If we compare
a parallel computational kernel with a sequential for-loop, this global id is
the iterator variable.
Line 8 performs the actual $f(x,y)$ calculation, and stores the result in the y
array, but only if the if-clause in line 7 is true.
As we have tens of thousands of threads running simultaneously on the CUDA
device, we only want to perform any array operations if we know that our current
global id is within the length of the array.
\\\\

\subsubsection{The CUDA main function}
We also need a main function to run the kernel:\\
Line 14 sets N to $1 << 20$ (or $2^{20}$).\\
Line 16 and 17 allocates memory for two arrays x and y in system memory.\\
Line 19 and 20 allocates memory for two arrays x and y on the CUDA device (the GPU.)\\
Line 22 to 25 initializes the arrays x and y with scalar values 1.0 and 2.0.\\
Line 27 and 28 copies our arrays from system memory to the corresponding arrays on
th CUDA device.\\
Line 31 executes the \texttt{saxpy} kernel.\\
Line 33 copies the result from CUDA device back to the y array in the system
memory. \\
The remaining lines frees the allocated memory, first from the CUDA device and
then from the system memory.

\begin{figure}[H]
  \centering
\begin{minted}[linenos]{cpp}
  #include <stdio.h>

__global__
void saxpy(int n, float a, float *x, float *y)
{
  int i = blockIdx.x*blockDim.x + threadIdx.x;
  if (i < n){
    y[i] = a*x[i] + y[i];
  }
}

int main(void)
{
  int N = 1<<20;
  float *x, *y, *d_x, *d_y;
  x = (float*)malloc(N*sizeof(float));
  y = (float*)malloc(N*sizeof(float));

  cudaMalloc(&d_x, N*sizeof(float)); 
  cudaMalloc(&d_y, N*sizeof(float));

  for (int i = 0; i < N; i++) {
    x[i] = 1.0f;
    y[i] = 2.0f;
  }

  cudaMemcpy(d_x, x, N*sizeof(float), cudaMemcpyHostToDevice);
  cudaMemcpy(d_y, y, N*sizeof(float), cudaMemcpyHostToDevice);

  // Perform SAXPY on 1M elements
  saxpy<<<(N+255)/256, 256>>>(N, 2.0f, d_x, d_y);

  cudaMemcpy(y, d_y, N*sizeof(float), cudaMemcpyDeviceToHost);

  cudaFree(d_x);
  cudaFree(d_y);
  free(x);
  free(y);
}
\end{minted}
  \caption{$ax + y$ in CUDA}
  \label{fig:cudasaxpy}
\end{figure}


\section{Futhark}
Whereas the CUDA program and kernel contained large amounts of memory handling
and bounds checking, a similar program written in Futhark spares us for a lot of
the manual labor above. Figure \ref{fig:futsaxpy} contains a Futhark program
that is semantically equivalent to the CUDA program, in regards to the
computational result.
\begin{figure}[H]
  \centering
\begin{lstlisting}[language=Futhark, numbers=left]
let saxpy (a : f32) (x : f32) (y : f32) : f32 =
  a*x+y

entry main =
  let N = 1<<20
  let a = 2f32
  let xs = replicate N 1f32
  let ys = replicate N 2f32
  let ys' = map2 (saxpy a) xs ys
  in ys'
  \end{lstlisting}
  \caption{$ax+y$ in Futhark}
  \label{fig:futsaxpy}
\end{figure}

Line 1 to 2 defines a function that takes three floats (\textit{a}, \textit{x}
and \textit{y}) and returns $ax+y$.  

Line 4 to 10 defines our main function.\\
In line 4 we use \texttt{entry} instead of \texttt{let} to tell the compiler
that \texttt{main} is an entry point in the compiled program. This means we can
call this function when we import the compiled program as a library, as opposed
to the function \texttt{saxpy}, which cannot be accessed as a library function.\\
Line 5 sets N to $1 << 20$ (or $2^{20}$).\\
Line 6 sets a to 2.0.\\
Line 7 uses the built-in function \texttt{replicate}\footnote{replicate has the
  type \texttt{int -> a -> []a}} to generate an N element
array of 1.0\\
Line 8 uses the built-in function \texttt{replicate} to generate an N element array of 2.0\\
Line 9 uses the built-in function \texttt{map2} to apply the curried function
(\texttt{saxpy a}) to the arrays xs and ys.

\texttt{map f xs} has the type \texttt{((a -> b) -> []a -> []b)}, and
returns the array of f applied to each element of xs.

\texttt{map2 f xs ys} is very similar, but has the type \texttt{((a -> b -> c) -> []a -> []b -> []c)}, and
applies f to the elements of xs and ys pairwise.\\
In this case, we are calling \texttt{map2} with the function \texttt{(saxpy a)},
which is just \texttt{saxpy} with the first argument $a$ already defined.

When we compare the program in figure \ref{fig:cudasaxpy} to the same program
written in Futhark (figure \ref{fig:futsaxpy}), we quickly see how Futhark's
high level declarative approach is simpler and less verbose than CUDA's.
The Futhark compiler does the heavy lifting, by parsing Fuhark source code and
generating OpenCL code and wrapping them in standalone C- or Python programs.

\section{\fsharp{}}
\fsharp{} is a high level multi-paradigm programming language in the .NET family.
\fsharp{}s syntax follows a classical functional programming style. For
instance, this means we can take (some programs) written in Futhark, and port
them to \fsharp{} in a way that very closely resembles the original Futhark
code.

Figure \ref{fig:fsharpsaxpy} shows the Futhark program from
\ref{sec:futharkintro} written in \fsharp{}.

\begin{figure}[H]
  \centering
\begin{minted}[linenos]{fsharp}
let saxpy (a : single) (x : single) (y : single) : single =
  a*x+y
  
let main =
  let n = 1<<<20
  let a = 2.0f
  let xs = array.replicate n 1.0f
  let ys = array.replicate n 2.0f
  let ys' = array.map2 (saxpy a) xs ys
  in ys'
  \end{minted}
  \caption{$ax+y$ in Futhark}
  \label{fig:fsharpsaxpy}
\end{figure}

\fsharp{} also supports object oriented programming, and has seamless
interoperability with the rest of the .NET language family. We can therefore
readily use \csharp{} libraries and classes in \fsharp{}, and vice versa.

\section*{\csharp{}}
\csharp{} 
\csharp{} 
\csharp{} 
\csharp{} 

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../thesis"
%%% End:
